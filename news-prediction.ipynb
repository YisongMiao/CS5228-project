{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from stop_words import get_stop_words\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4356, 7) (484, 6)\n"
     ]
    }
   ],
   "source": [
    "training_data_filepath = 'data/train.csv'\n",
    "test_data_filepath = 'data/test.csv'\n",
    "train_dataframe = pd.read_csv(training_data_filepath)\n",
    "test_dataframe = pd.read_csv(test_data_filepath)\n",
    "print(train_dataframe.shape, test_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id                                                    1\n",
      "title         Forex - Pound drops to one-month lows against ...\n",
      "url           http://www.nasdaq.com/article/forex-pound-drop...\n",
      "publisher                                                NASDAQ\n",
      "hostname                                         www.nasdaq.com\n",
      "timestamp                                              1.39e+12\n",
      "category                                                      4\n",
      "Name: 0, dtype: object\n",
      "['Forex', 'Pound', 'drops', 'to', 'one', 'month', 'lows', 'against', 'euro']\n"
     ]
    }
   ],
   "source": [
    "#print(train_dataframe['title'])\n",
    "print(train_dataframe.iloc[0])\n",
    "a = re.findall(r'\\w+', train_dataframe.iloc[0]['title'])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "print(clf.predict(X[2:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4547\n",
      "['sort', 'evil', 'appeal', 'highway', 'abercrombie', 'suvs', 'bet', 'british', 'wsj', 'billion', 'saw', 'traders', 'tanks', 'relief', 'cabinet', 'buyout', 'upper', 'detected', 'smit', 'recalls', 'temporary', 'respiratory', 'hispanics', 'based', 'couple', 'ousts', 'ppaca', 'zero', 'greets', 'enforcement', 'inaccuracies', 'commerce', 'bromund', 'gun', 'actiev', 'toyota', 'last', 'majority', 'lulac', 'downs', 'packaged', 'planes', 'justice', 'fielded', 'ones', 'defaults', 'spending', 'roll', 'fade', 'interline', 'snub', 'cold', 'takeover', 'core', 'mutual', 'suddenly', 'seller', 'execute', 'loses', 'exec', 'send', 'kingston', 'latest', 'birthday', 'outfitters', 'dublin', 'tux', 'liability', 'ceo', 'better', 'cooperation', 'libyan', 'due', 'congress', 'made', 'right', 'choppy', 'boosting', 'cheese', 'hukou', 'win', 'hit', 'rental', 'words', 'eurosceptics', 'brent', 'guys', 'karpeles', 'back', 'bmo', 'unknown', 'signing', 'outpace', 'thumbs', 'increases', 'quiz', 'unlocks', 'plosser', 'rejig', 'aims']\n"
     ]
    }
   ],
   "source": [
    "def process_title(train_dataframe):\n",
    "    # TODO: remove stop words\n",
    "    # TODO: Normalize\n",
    "    # Add number feature\n",
    "    vocabulary_list = []\n",
    "    for i in range(train_dataframe.shape[0]):\n",
    "        a = re.findall(r'[A-Za-z]+', train_dataframe.iloc[i]['title'])\n",
    "        a = [item.lower() for item in a if (len(item) > 1 and item.lower() not in stop_words)]\n",
    "        vocabulary_list += a\n",
    "    vocabulary_list = list(set(vocabulary_list))\n",
    "    return vocabulary_list\n",
    "\n",
    "title_vocabulary_list = process_title(train_dataframe)\n",
    "print(len(title_vocabulary_list))\n",
    "print(title_vocabulary_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368\n"
     ]
    }
   ],
   "source": [
    "def process_publisher(train_dataframe):\n",
    "    vocabulary_list = []\n",
    "    for i in range(train_dataframe.shape[0]):\n",
    "        vocabulary_list.append(train_dataframe.iloc[i]['publisher'])\n",
    "    vocabulary_list = list(set(vocabulary_list))\n",
    "    return vocabulary_list\n",
    "\n",
    "publisher_vocabulary_list = process_publisher(train_dataframe)\n",
    "print(len(publisher_vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_instance(instance, title_vocabulary_list, publisher_vocabulary_list):\n",
    "    # TODO: make it compatible for all cases, now only for title + publisher?\n",
    "    output_array = np.zeros((1, (len(title_vocabulary_list) + len(publisher_vocabulary_list))))\n",
    "    \n",
    "    a = re.findall(r'[A-Za-z]+', instance['title'])\n",
    "    a = [item.lower() for item in a if (len(item) > 1 and item.lower() not in stop_words and item in title_vocabulary_list)]\n",
    "    title_counter = Counter(a)\n",
    "    for item in title_counter:\n",
    "        output_array[0, title_vocabulary_list.index(item)] = title_counter[item]\n",
    "    \n",
    "    current_publisher = instance['publisher']\n",
    "    if current_publisher in publisher_vocabulary_list:\n",
    "        output_array[0, len(title_vocabulary_list) + publisher_vocabulary_list.index(current_publisher)] = 1\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(dataframe, title_vocabulary_list, publisher_vocabulary_list):\n",
    "    # TODO: make it compatible for all cases, now only for title + publisher?\n",
    "    output_array = np.zeros((dataframe.shape[0], (len(title_vocabulary_list) + len(publisher_vocabulary_list))))\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        instance = dataframe.iloc[i]\n",
    "        output_array[i, :] = vectorize_instance(instance, title_vocabulary_list, publisher_vocabulary_list)\n",
    "\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4356,)\n",
      "(4356, 5915)\n"
     ]
    }
   ],
   "source": [
    "train_Y = train_dataframe['category']\n",
    "print(train_Y.shape)\n",
    "train_X = vectorize_dataset(train_dataframe, title_vocabulary_list, publisher_vocabulary_list)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 5915)\n"
     ]
    }
   ],
   "source": [
    "test_Y = test_dataframe['category']\n",
    "print(test_Y.shape)\n",
    "test_X = vectorize_dataset(test_dataframe, title_vocabulary_list, publisher_vocabulary_list)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
